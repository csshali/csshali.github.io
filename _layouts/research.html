---
layout: default
---

{{ content }}

<div class="row g-5 mb-5">
  <div>
    <h3 class="fw-bold border-bottom pb-3 mb-5">Research</h3>
    <div class="col-md-10">
      <i style='font-size:24px' class='fas'>&#xf5dc;</i><p4 class="fw-bold"> Knowledge-enhanced Language Models </p4>
      <br /><br />


      <i style='font-size:24px' class='fas'>&#xf5dc;</i><p4 class="fw-bold"> Cascade Growth Prediction </p4>
    
      <p>Brief Intro: Constructed random forest to predict Twitter cascades growth with an emphasis of feature engineering to generate content, temporal, root, reshare and structural features. Implemented the whole pipeline with Python and BigQuery.  </p>
      <div class="col-md-6">
    <img src="{{ site.github.url }}/assets/img/cascade.png" alt="Cascades predict" width="75%" style="margin-left:100px">
      </div>
      <br /><br />

      <i style='font-size:24px' class='fas'>&#xf5dc;</i><p4 class="fw-bold"> Graph Neural Networks </p4>
      <br>
      <p> - Adapted prompt tuning to GNNs by injecting continuous vectors with trainable parameters to the embedding space of graphs to close the optimization gap between pretext and downstream tasks. </p>
      <p> - Developed a communication-efficient framework for federated graph learning by compressing graph architectures. </p>
      <br /><br />

      <i style='font-size:24px' class='fas'>&#xf5dc;</i><p4 class="fw-bold"> Edge AI & Efficient AI </p4>
      <br>
      <p> - Developed light-weight deep neural network (DNN) models for edge applications (e.g. keyword spotting, image classification) by quantization/ binarization to reduce DNNsâ€™ memory and computation cost, and enabled their deployment on resource-constrain edge devices/ chiplets. </p>
      <p> - Researched on DNN compression techniques (quantization, sparsification, knowledge distillation) to reduce memory, storage and footprint consumption for DNNs but remain their accuracy. </p>
      <p> - Algo-hardware co-design: Designed hardware-aware training schemes to integrate hardware limitations (e.g. memory, computation, energy) and deficiency (e.g. noise) in end-to-end DNN training to preserve DNN model precision. </p>
    <br /><br />

      <i style='font-size:24px' class='fas'>&#xf5dc;</i><p4 class="fw-bold"> Ensemble Learning for FinTech and RegTech Applications </p4>
<br /><br />

      <i style='font-size:24px' class='fas'>&#xf5dc;</i><p4 class="fw-bold"> Anomal Detection on Time-series Data </p4>
<br /><br />

      <i style='font-size:24px' class='fas'>&#xf5dc;</i><p4 class="fw-bold"> Deep Reinforcement Learning for Resource Management and Optimization </p4>
<br /><br />
      </div>


  </div>
</div>
